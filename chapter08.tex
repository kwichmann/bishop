\documentclass[12pt, a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\graphicspath{ {img/} }
\DeclareGraphicsExtensions{.png}

\title{Chapter 8 - Graphical models}
\author{Kristian Wichmann}

\begin{document}
\maketitle

\section{Exercise 8.1}
The parent node factorization for a Bayesian network is:
\begin{equation}
p(\mathbf{x})=\prod_i p(x_i|\textrm{pa}_i)
\label{parent_node_factorization}
\end{equation}
Show that if each of the conditional distributions are normalized, then so is the joint distribution.

\subsection{Solution}
The assumption is, that for any combination of values of $\textrm{pa}_i$, we have:
\begin{equation}
\int p(x_i|\textrm{pa}_i)\ d\mu_i(x_i)=1
\label{normalization_assumption}
\end{equation}
Here $\mu_i$ is the relevant dominating measure for the probability density function for $x_i$.

We wish to calculate:
\begin{equation}
\int p(\mathbf{x})\ d\mu_1(x_1)\cdots d\mu_K(x_K)
\end{equation}
Using equation \ref{parent_node_factorization} this can be written:
\begin{equation}
\int p(x_K|\textrm{pa}_K)\cdots p(x_1|\textrm{pa}_1) d\mu_1(x_1)\cdots d\mu_K(x_K)
\end{equation}
Now using the assumption from equation \ref{normalization_assumption} for $i=1$, the inner innermost integration turns to one. Doing so for $i=2$ and so on up to $i=K$, we get that the entire integral is equal to 1 as desired.

\section{Exercise 8.2}
Show that the property of there being no directed cycles in a directed graphs follows from the statement that there exists an ordering of the nodes, such that there are no links to a lower-numbered node.

\subsection{Solution}
Let $x_1, x_2,\dots, x_K$ be such an ordering. Assume that a directed cycle exists
\begin{equation}
x_{i_1}\rightarrow x_{i_2}\rightarrow\cdots\rightarrow x_{i_N}=x_{i_1}
\end{equation}
By the ordering assumption we must have:
\begin{equation}
i_1<i_2<\cdots<i_n
\end{equation}
But since $i_1=i_n$ we have $i_1<i_1$, which is a contradiction.

\section{Exercise 8.3}
The joint distribution of three binary random variables is summed up in table \ref{table:e8_3}.

\begin{table}[b]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$a$ & $b$ & $c$ & $p(a,b,c)$ \\ \hline
0 & 0 & 0 & 0.192    \\ \hline
0 & 0 & 1 & 0.144    \\ \hline
0 & 1 & 0 & 0.048    \\ \hline
0 & 1 & 1 & 0.216    \\ \hline
1 & 0 & 0 & 0.192    \\ \hline
1 & 0 & 1 & 0.064    \\ \hline
1 & 1 & 0 & 0.048    \\ \hline
1 & 1 & 1 & 0.096    \\ \hline
\end{tabular}
\caption{Joint distribution for exercise 8.3 and 8.4}
\label{table:e8_3}
\end{table}

Show that the variables $a$ and $b$ are marginally dependent, but that they become independent when conditioning on $c$.

\subsection{Solution}
Margin dependence amounts to showing that $p(a)p(b)\neq p(a,b)$. The two first two distributions are found by summing over when each variable is 0 and 1, respectively:
\begin{align}
p(a=0)=0.192+0.144+0.048+0.216=& 0.6 \\
p(a=1)=0.192+0.064+0.048+0.096=& 0.4 \\
p(b=0)=0.192+0.144+0.192+0.064=& 0.592 \\
p(b=1)=0.048+0.216+0.048+0.096=& 0.408
\end{align}
The distribution of $p(a,b)$ is found by summing over the four possibilities for the combinations of $a$ and $b$:
\begin{align}
p(a=0, b=0)=0.192+0.144=& 0.336 \\
p(a=0, b=1)=0.048+0.216=& 0.264 \\
p(a=1, b=0)=0.192+0.064=& 0.256 \\
p(a=1, b=0)=0.048+0.096=& 0.144
\end{align}
It suffices to show the inequality for one case:
\begin{equation}
p(a=0)p(b=0)=0.6\cdot 0.592=0.3552\neq 0.336=p(a=0,b=0)
\end{equation}

Now, we condition on $c$. I.e. we wish to show that $p(a|c)p(b|c)=p(a,b|c)$. To calculate the conditional probabilities, we need the distribution of $c$:
\begin{align}
p(c=0)=0.192+0.048+0.192+0.048=& 0.48 \\
p(c=1)=0.144+0.216+0.064+0.096=& 0.52
\end{align}
The conditional probablilities for $a$ are:
\begin{align}
p(a=0|c=0)=\frac{p(a=0,c=0)}{p(c=0)}=\frac{0.192+0.048}{0.48}=& 0.5 \\
p(a=1|c=0)=\frac{p(a=1,c=0)}{p(c=0)}=\frac{0.192+0.048}{0.48}=& 0.5 \\
p(a=0|c=1)=\frac{p(a=0,c=1)}{p(c=1)}=\frac{0.144+0.216}{0.52}=& \frac{9}{13}\approx 0.692 \\
p(a=1|c=1)=\frac{p(a=1,c=1)}{p(c=1)}=\frac{0.064+0.096}{0.52}=& \frac{4}{13}\approx 0.308
\end{align}
And for $b$:
\begin{align}
p(b=0|c=0)=\frac{p(b=0,c=0)}{p(c=0)}=\frac{0.192+0.192}{0.48}=& 0.8 \\
p(b=1|c=0)=\frac{p(b=1,c=0)}{p(c=0)}=\frac{0.048+0.048}{0.48}=& 0.2 \\
p(b=0|c=1)=\frac{p(b=0,c=1)}{p(c=1)}=\frac{0.144+0.064}{0.52}=& 0.4 \\
p(b=1|c=1)=\frac{p(b=1,c=1)}{p(c=1)}=\frac{0.216+0.096}{0.52}=& 0.6
\end{align}
And the conditionals $p(a,b|c)$:
\begin{align}
p(a=0,b=0|c=0)=\frac{p(a=0,b=0,c=0}{p(c=0)}=\frac{0.192}{0.48}=& 0.4 \\
p(a=0,b=1|c=0)=\frac{p(a=0,b=1,c=0}{p(c=0)}=\frac{0.048}{0.48}=& 0.1 \\
p(a=1,b=0|c=0)=\frac{p(a=1,b=0,c=0}{p(c=0)}=\frac{0.192}{0.48}=& 0.4 \\
p(a=1,b=1|c=0)=\frac{p(a=1,b=1,c=0}{p(c=0)}=\frac{0.048}{0.48}=& 0.1 \\
p(a=0,b=0|c=1)=\frac{p(a=0,b=0,c=1}{p(c=1)}=\frac{0.144}{0.52}=& \frac{18}{65}\approx 0.277 \\
p(a=0,b=1|c=1)=\frac{p(a=0,b=1,c=1}{p(c=1)}=\frac{0.216}{0.52}=& \frac{27}{65}\approx 0.415 \\
p(a=1,b=0|c=1)=\frac{p(a=1,b=0,c=1}{p(c=1)}=\frac{0.064}{0.52}=& \frac{8}{65}\approx 0.123 \\
p(a=1,b=1|c=1)=\frac{p(a=1,b=1,c=1}{p(c=1)}=\frac{0.096}{0.52}=& \frac{12}{65}\approx 0.185 
\end{align}

\begin{table}[t]
\centering
\begin{tabular}{ccclccc}
\multicolumn{3}{c}{$p(a|c=0)p(b|c=0)$}                                                &                       & \multicolumn{3}{c}{$p(a,b|c=0)$}                                                     \\ \cline{2-3} \cline{6-7} 
\multicolumn{1}{c|}{}       & \multicolumn{1}{c|}{$b=0$}   & \multicolumn{1}{c|}{$b=1$}   &                       & \multicolumn{1}{c|}{}      & \multicolumn{1}{c|}{$b=0$}   & \multicolumn{1}{c|}{$b=1$}   \\ \cline{1-3} \cline{5-7} 
\multicolumn{1}{|c|}{$a=0$}   & \multicolumn{1}{c|}{$0.5\cdot 0.8$}      & \multicolumn{1}{c|}{$0.5\cdot 0.2$}      & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$a=0$}   & \multicolumn{1}{c|}{0.4}      & \multicolumn{1}{c|}{0.1}      \\ \cline{1-3} \cline{5-7} 
\multicolumn{1}{|c|}{$a=1$}   & \multicolumn{1}{c|}{$0.5\cdot 0.8$}      & \multicolumn{1}{c|}{$0.5\cdot 0.2$}      & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$a=1$}   & \multicolumn{1}{c|}{0.4}      & \multicolumn{1}{c|}{0.1}      \\ \cline{1-3} \cline{5-7} 
\multicolumn{3}{c}{$p(a|c=1)p(b|c=1)$}                                                & \multicolumn{1}{c}{}  & \multicolumn{3}{c}{$p(a,b|c=1)$}                                                     \\ \cline{2-3} \cline{6-7} 
\multicolumn{1}{c|}{}       & \multicolumn{1}{c|}{$b=0$} & \multicolumn{1}{c|}{$b=1$} & \multicolumn{1}{c}{}  & \multicolumn{1}{c|}{}      & \multicolumn{1}{c|}{$b=0$} & \multicolumn{1}{c|}{$b=1$} \\ \cline{1-3} \cline{5-7} 
\multicolumn{1}{|c|}{$a=0$} & \multicolumn{1}{c|}{$\frac{9}{13}\cdot 0.4$}      & \multicolumn{1}{c|}{$\frac{9}{13}\cdot 0.6$}      & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{$a=0$} & \multicolumn{1}{c|}{$\frac{18}{65}$}      & \multicolumn{1}{c|}{$\frac{27}{65}$}      \\ \cline{1-3} \cline{5-7} 
\multicolumn{1}{|c|}{$a=1$} & \multicolumn{1}{c|}{$\frac{4}{13}\cdot 0.4$}      & \multicolumn{1}{c|}{$\frac{4}{13}\cdot 0.6$}      & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{$a=1$} & \multicolumn{1}{c|}{$\frac{8}{65}$}      & \multicolumn{1}{c|}{$\frac{12}{65}$}      \\ \cline{1-3} \cline{5-7} 
\end{tabular}
\caption{Conditioning on $c$}
\label{table:e_83_conditional}
\end{table}

Now we can check if $p(a|c)p(b|c)=p(a,b|c)$. See table \ref{table:e_83_conditional}. Now the tables in each row contains the same probabilities, as desired.

\end{document}