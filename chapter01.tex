\documentclass[12pt, a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\graphicspath{ {img/} }
\DeclareGraphicsExtensions{.png, .jpg}

\title{Chapter 1 - Introduction}
\author{Kristian Wichmann}

\begin{document}
\maketitle

\section{Exercise 1.1}
Suppose we are given a training set, of $N$ observation points:
\begin{equation}
\mathbf{x}=(x_1,\cdots,x_n)^t,\quad \mathbf{t}=(t_1,\cdots,t_n)^t
\end{equation}
We think of the $t$'s as being dependent on the $x$'es: $t=t(x)$. We wish to devise a model through polynomial curve fitting of order $M$. I.e. our model is of the form:
\begin{equation}
y(x,\mathbf{w})=w_0+w_1 x+w_2 x^2+\cdots+w_M x^M=\sum_{j=0}^M w_j x^j
\end{equation} 
Here the parameters $\mathbf{w}=(w_0,\cdots,w_M)^t$ are known as the weights. We wish to find the model which minimizes the following error function:
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^N\left[y(x_n,\mathbf{w})-t_n)\right]^2
\end{equation}

\subsection{Solution}
To minimize the error function, we differentiate with respect to the $i$'th weight, using the chain rule:
\begin{equation}
\label{error_deriv}
\frac{\partial E}{\partial w_i}=\frac{1}{2}\sum_{n=1}^N\frac{\partial}{\partial w_i}\left[y(x_n,\mathbf{w})-t_n)\right]^2=\sum_{n=1}^N\left[y(x_n,\mathbf{w})-t_n)\right]\frac{\partial y(x_n,\mathbf{w})}{\partial w_i}
\end{equation}
The last derivative is:
\begin{equation}
\frac{\partial y(x_n,\mathbf{w})}{\partial w_i}=\frac{\partial}{\partial w_i}\sum_{j=0}^M w_j x_n^j=\sum_{j=0}^M \delta_{ij}x_n^j=x_n^i
\end{equation}
Now equation \ref{error_deriv} becomes:
\begin{equation}
\frac{\partial E}{\partial w_i}=\sum_{n=1}^N\left[\sum_{j=0}^M w_j x_n^j-t_n\right]x_n^i=\sum_{n=1}^N\left[\sum_{j=0}^M w_j x_n^{i+j}-t_n x_n^i\right]
\end{equation}
Now define:
\begin{equation}
A_{ij}=\sum_{n=1}^N x_n^{i+j}\quad T_i=\sum_{n=1}^N t_n x_n^i
\end{equation}
Then we can rewrite:
\begin{equation}
\label{deriv_rewritten}
\frac{\partial E}{\partial w_i}=\sum_{j=0}^M A_{ij}w_j-T_i
\end{equation}
Setting this derivative equal to zero, we get:
\begin{equation}
\sum_{j=0}^M A_{ij}w_j=T_i
\end{equation}

\section{Exercise 1.2}
Consider the same problem as last exercise, but with an additional regularization term added to the error function:
\begin{equation}
\tilde{E}(\mathbf{w})=\frac{1}{2}\sum_{n=1}^N\left[y(x_n,\mathbf{w})-t_n)\right]^2+\frac{\lambda}{2}||\mathbf{w}||^2
\end{equation}
We wish to find the equations the weights must satisfy in this case.

\subsection{Solution}
The derivative of the error function is modified:
\begin{equation}
\frac{\partial\tilde{E}}{\partial w_i}=\frac{\partial E}{\partial w_i}+\frac{\partial}{\partial w_i}\left[\frac{\lambda}{2}||\mathbf{w}||^2\right]
\end{equation}
The last correction term is:
\begin{equation}
\frac{\lambda}{2}\frac{\partial}{\partial w_i}(w_0^2+w_1^2+\cdots+w_M^2)=\lambda w_i
\end{equation}
According to equation \ref{deriv_rewritten} we have:
\begin{equation}
\frac{\partial\tilde{E}}{\partial w_i}=\sum_{j=0}^M A_{ij}w_j-T_i+w_i
\end{equation}
Setting this equal to zero we get:
\begin{equation}
\sum_{j=0}^M A_{ij}w_j+\lambda w_i=T_i
\end{equation}
We may absorb the $\lambda$ term into the $A$ matrix by the following modification:
\begin{equation}
\tilde{A}_{ij}=A_{ij}+\lambda\delta_{ij}
\end{equation}
Then we get:
\begin{equation}
\sum_{j=0}^M\tilde{A}_{ij}w_j=T_i
\end{equation}

\section{Exercise 1.3}

\begin{table}
\centering
\caption{Content of the boxes (Exercise 1.3)}
\label{table:boxes}
\begin{tabular}{l|l|l|l|}
\cline{2-4}
\multicolumn{1}{c|}{}         & \multicolumn{1}{c|}{r} & b & g \\ \hline
\multicolumn{1}{|l|}{Apples}  & 3                      & 1 & 3 \\ \hline
\multicolumn{1}{|l|}{Oranges} & 4                      & 1 & 3 \\ \hline
\multicolumn{1}{|l|}{Limes}   & 3                      & 0 & 4 \\ \hline
\end{tabular}
\end{table}

We have three colored boxes, $r$ (red), $b$ (blue), and $g$ (green). The contents of the three boxes are shown in table \ref{table:boxes}. A box is chosen at random according to the following probability distribution:
\begin{equation}
p(r)=\frac{1}{5},\quad p(b)=\frac{1}{5},\quad p(g)=\frac{3}{5}
\end{equation}
Next, a random piece of fruit (equal chance for each piece) is chosen from the box.

\begin{enumerate}
\item What is the probability of the fruit being an apple?
\item Given that the selected fruit is an orange, what is that probability that the green box was picked?
\end{enumerate}

\subsection{Solution 1}
The probability of getting an apple (A) is:
\begin{equation}
p(A)=p(A|r)p(r)+p(A|b)p(b)+p(A|g)p(g)
\end{equation}
Inserting:
\begin{equation}
p(A)=\frac{3}{10}\cdot\frac{1}{5}+\frac{1}{2}\cdot\frac{1}{5}+\frac{3}{10}\cdot\frac{3}{5}=\frac{3}{50}+\frac{1}{10}+\frac{9}{50}=\frac{3+5+9}{50}=\frac{17}{50}=34\%
\end{equation}

\subsection{Solution 2}
We wish to find $p(g|O)$, where 'O' is short for orange. For this we need Bayes' rule:
\begin{equation}
p(g|O)=\frac{p(O|g)p(g)}{p(O)}
\end{equation}
The denominator can be found similarly to problem 1:
\begin{equation}
p(O)=p(O|r)p(r)+p(O|b)p(b)+p(O|g)p(g)
\end{equation}
Inserting:
\begin{equation}
p(O)=\frac{4}{10}\cdot\frac{1}{5}+\frac{1}{2}\cdot\frac{1}{5}+\frac{3}{10}\cdot\frac{3}{5}=\frac{4+5+9}{50}=\frac{18}{50}=36\%
\end{equation}
Now we have:
\begin{equation}
p(g|O)=\frac{3/10\cdot 3/5}{18/50}=\frac{9\cdot 50}{50\cdot 18}=\frac{9}{18}=\frac{1}{2}
\end{equation}

\section{Exercise 1.4}
Let $p_x(x)$ be a probability density function for a continuous variable $x$. Consider a variable transformation $x=g(y)$. The pdf for the transformed variable is then:
\begin{equation}
p_y(y)=p_x(x)\left|\frac{dx}{dy}\right|=p_x(g(y))|g'(y)|
\end{equation} 
Show that in general, a non-linear transformation will change the location of a maximum of the pdf, while a linear one will not.

\subsection{Solution}
If $p_x$ has a maximum at $x=\hat{x}$ then we must have:
\begin{equation}
\frac{d p_x(\hat{x})}{dx}=0
\end{equation}
Now, consider the derivative of $p_y$:
\begin{equation}
\frac{d p_y(y)}{dy}=\frac{d p_x}{dx}\frac{dg}{dy}|g'(y)|+p_x(g(y))\textrm{sgn}(g'(y))g''(y)
\end{equation}
If $y=g(\hat{x})$, the first term is zero, but the second need not be. Therefore, the location of a maximum will generally move. However, for a linear transformation the second derivative is zero, and so the second term becomes zero as well.

\section{Exercise 1.5}
The variance of a continuous random variable $f(x)$ is defined as:
\begin{equation}
\textrm{var}[f(x)]=\mathbb{E}[(f(x)-\mathbb{E}[f(x)])^2]
\end{equation}
Show that this may be written:
\begin{equation}
\textrm{var}[f(x)]=\mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2
\end{equation}

\subsection{Solution}
Expand the square inside the expectation value:
\begin{equation}
(f(x)-\mathbb{E}[f(x)])^2=f(x)^2+\mathbb{E}[f(x)]^2-2f(x)\mathbb{E}[f(x)]
\end{equation}
The expectation values is:
\begin{equation}
\mathbb{E}[f(x)^2]+\mathbb{E}[f(x)]^2-2\mathbb{E}[f(x)]\cdot\mathbb{E}[f(x)]=\mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2
\end{equation}

\section{Exercise 1.6}
Show that two independent random variable $x$ and $y$ have zero covariance.

\subsection{Solution}
The covariance can be written:
\begin{equation}
\int\int(x-\mathbb{E}[x])(y-\mathbb{E}[y])p(x,y)dx\ dy
\end{equation}
Here $p(x,y)$ is the joint distribution function of $x$ and $y$. But since these are independent $p(x,y)=p(x)p(y)$. So this integral splits into two parts:
\begin{equation}
\int\int(x-\mathbb{E}[x])(y-\mathbb{E}[y])p(x)p(y)dx\ dy=\int(x-\mathbb{E}[x])dx\cdot\int(y-\mathbb{E}[y])dy
\end{equation}
Each of these factors is zero, and therefore so is the covariance.

\section{Exercise 1.7}
Show that:
\begin{equation}
I=\int_{-\infty}^\infty\exp\left(-\frac{1}{2\sigma^2}x^2\right)dx=\sqrt{2\pi\sigma^2}
\end{equation}

\subsection{Solution}
Consider the square of $I$:
\begin{equation}
I^2=\int_{-\infty}^\infty\exp\left(-\frac{1}{2\sigma^2}x^2\right)dx\cdot\int_{-\infty}^\infty\exp\left(-\frac{1}{2\sigma^2}y^2\right)dy
\end{equation}
Rearrange the order:
\begin{align}
I^2=&\int_{-\infty}^\infty\int_{-\infty}^\infty\exp\left(-\frac{1}{2\sigma^2}x^2\right)\exp\left(-\frac{1}{2\sigma^2}y^2\right)dx\ dy=\\
&\int_{-\infty}^\infty\int_{-\infty}^\infty\exp\left(-\frac{1}{2\sigma^2}(x^2+y^2)\right)dx\ dy
\end{align}
The may be regarded as an integral over the plane. We may now shift to polar coordinates, noting that $x^2+y^2=r^2$ and $dx\ dy=r\ dr\ d\theta$:
\begin{equation}
I^2=\int_0^{2\pi}\int_0^\infty\exp\left(-\frac{r^2}{2\sigma^2}\right)r\ dr\ d\theta=2\pi\int_0^\infty\exp\left(-\frac{r^2}{2\sigma^2}\right)r\ dr
\end{equation}
For the $r$-integral, substitute $u=r^2$. Then $du/dr=2r$, and so $dr=\frac{1}{2r}du$:
\begin{equation}
\int_0^\infty\exp\left(-\frac{r^2}{2\sigma^2}\right)r\ dr=\int_0^\infty\exp\left(-\frac{u}{2\sigma^2}\right)r\frac{1}{2r}du=\frac{1}{2}\int_0^\infty\exp\left(-\frac{u}{2\sigma^2}\right)du
\end{equation}
This is elemental to integrate:
\begin{equation}
\frac{1}{2}\left[-2\sigma^2\exp\left(-\frac{u}{2\sigma^2}\right)\right]_0^\infty=\sigma^2
\end{equation}
Hence $I^2=2\pi\sigma^2$, and therefore $I=\sqrt{2\pi\sigma^2}$.

\section{Exercise 1.8}
The univariate Gaussian distribution with mean $\mu$ and variance $\sigma^2$ has the pdf:
\begin{equation}
\mathcal{N}(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}
\begin{enumerate}
\item Show that the expectation value is actually $\mu$.
\item Show that $\mathbb{E}[x^2]=\mu^2+\sigma^2$.
\item Show that the variance is actually $\sigma^2$.
\end{enumerate}

\subsection{Solution 1}
We need to evaluate the integral:
\begin{equation}
\mathbb{E}[x]=\int_{-\infty}^\infty\mathcal{N}(x|\mu,\sigma^2)x\ dx=\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)x\ dx
\end{equation}
Now apply the substitution $u=x-\mu$. This means $du=dx$. Also $x=u+\mu$:
\begin{equation}
\mathbb{E}[x]=\frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^\infty\exp\left(-\frac{u^2}{2\sigma^2}\right)(u+\mu)du
\end{equation}
We may split the integral into two:
\begin{equation}
\int_{-\infty}^\infty\exp\left(-\frac{u^2}{2\sigma^2}\right)u\ du+\mu\int_{-\infty}^\infty\exp\left(-\frac{u^2}{2\sigma^2}\right)du
\end{equation}
The first is an uneven function, and so the integral is zero. The second is equal to $\mu\sqrt{2\pi\sigma^2}$ according to the last exercise. So all in all we get:
\begin{equation}
\mathbb{E}[x]=\mu
\end{equation} 

\subsection{Solution 2}
The normalization condition is:
\begin{equation}
\int_{-\infty}^\infty\mathcal{N}(x|\mu,\sigma^2)\ dx=1
\end{equation}
Now differentiate on both sides of the equation with respect to $\sigma^2$. The right side is obvious zero, while the left side is:
\begin{align}
\int_{-\infty}^\infty\frac{\partial}{\partial(\sigma^2)}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)dx&=\\
\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}\frac{-1}{2(\sigma^2)^{3/2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)dx&+\\
\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\frac{-(x-\mu)^2}{2}\frac{-1}{(\sigma^2)^2}dx&=\\
-\frac{1}{\sigma^2}\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)dx&+\\
\frac{1}{\sigma^4}\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)(x-\mu)^2\ dx&
\end{align}
The first integral is simply 1, while the second can be split further into three by expanding the square:
\begin{equation}
(x-\mu)^2=x^2+\mu^2-2x\mu
\end{equation}
The $x^2$-integral is what we're trying to calculate. The $mu^2$-integral is a constant times 1 again. And finally the mixed term can be found from part 1. All in all:
\begin{align}
-\frac{1}{\sigma^2}+\frac{1}{\sigma^4}\left[\mathbb{E}[x^2]+\mu^2-2\mu\cdot\mu\right]&=0\Leftrightarrow\\
-1+\frac{1}{\sigma^2}\left[\mathbb{E}[x^2]-\mu^2\right]&=0\Leftrightarrow\\
\frac{1}{\sigma^2}\left[\mathbb{E}[x^2]-\mu^2\right]&=1\Leftrightarrow\\
\mathbb{E}[x^2]-\mu^2&=\sigma^2\Leftrightarrow\\
\mathbb{E}[x^2]&=\mu^2+\sigma^2
\end{align}

\subsection{Solution 3}
We can now find the variance by applying the formula from the exercise above, and using the previous solutions:
\begin{equation}
\textrm{var}[x]=\mathbb{E}[x^2]-\mathbb{E}[x]^2=\mu^2+\sigma^2-\mu^2=\sigma^2
\end{equation}

\section{Exercise 1.9}
Show that the mode of the univariate Gaussian distribution $\mathcal{N}(x|\mu,\sigma^2)$ has mode $\mu$. Similarly, show the that multivariate Gaussian $\mathcal{N}(\mathbf{x}|\mathbf{\mu},\mathbf{\Sigma})$ has mode $\mathbf{\mu}$.

\subsection{Solution}
The mode is where the pdf is maximal. Differentiate with respect to $x$:
\begin{equation}
\frac{\partial}{\partial x}\mathcal{N}(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\frac{\partial}{\partial x}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}
Using the chain rule this is equal to:
\begin{equation}
\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\left(-\frac{2}{2\sigma^2}(x-\mu)\right)
\end{equation}
This may only be equal to zero if $x-\mu=0\Leftrightarrow x=\mu$.

For the multivariate Gaussian in $D$ dimensions, the pdf is:
\begin{equation}
\mathcal{N}(\mathbf{x}|\mathbf{\mu},\mathbf{\Sigma})=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\mathbf{\Sigma}|}\exp\left\{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^t\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right\}
\end{equation}
Again, the normalization constant doesn't matter, and since the exponential is monotonically increasing, this comes down to minimizing the quadratic form. Written in coordinate form, this is:
\begin{equation}
-\frac{1}{2}\sum_{i=1}^D\sum_{j=1}^D(\mathbf{x}-\mathbf{\mu})_i\mathbf{\Sigma}^{-1}_{ij}(\mathbf{x}-\mathbf{\mu})_j=-\frac{1}{2}\sum_{i=1}^D\sum_{j=1}^D(x_i-\mu_i)\mathbf{\Sigma}^{-1}_{ij}(x_j-\mu_j)
\end{equation}
Now differentiate with respect to $x_k$:
\begin{align}
\frac{\partial}{\partial x_k}\left[-\frac{1}{2}\sum_{i=1}^D\sum_{j=1}^D(x_i-\mu_i)\mathbf{\Sigma}^{-1}_{ij}(x_j-\mu_j)\right]&=\\
-\frac{1}{2}\sum_{i=1}^D\sum_{j=1}^D\frac{\partial}{\partial x_k}(x_i-\mu_i)\mathbf{\Sigma}^{-1}_{ij}(x_j-\mu_j)&+\\
-\frac{1}{2}\sum_{i=1}^D\sum_{j=1}^D(x_i-\mu_i)\mathbf{\Sigma}^{-1}_{ij}\frac{\partial}{\partial x_k}(x_j-\mu_j)&=\\
-\frac{1}{2}\sum_{i=1}^D\sum_{j=1}^D\left[\delta_{ik}\mathbf{\Sigma}^{-1}_{ij}(x_j-\mu_j)+(x_i-\mu_i)\mathbf{\Sigma}^{-1}_{ij}\delta_{jk}\right]
\end{align}
Cancelling the deltas and renaming the first summation this is:
\begin{equation}
-\frac{1}{2}\sum_{i=1}^D\left[\mathbf{\Sigma}^{-1}_{ki}(x_i-\mu_i)+\mathbf{\Sigma}^{-1}_{ik}(x_i-\mu_i)\right]
\end{equation}
Since the covariance matrix is symmetric, so is the inverse, and we get:
\begin{equation}
-\sum_{i=1}^D\mathbf{\Sigma}^{-1}_{ki}(x_i-\mu_i)
\end{equation}
Back in matrix form this should be zero:
\begin{equation}
-\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})=0
\end{equation}
Now multiply both sides by $\mathbf{\Sigma}$ to get that $\mathbf{x}-\mathbf{\mu}=0\Leftrightarrow\mathbf{x}=\mathbf{\mu}$.

\section{Exercise 1.10}
Assuming the random variables $x$ and $z$ to independent, show that:
\begin{enumerate}
\item $\mathbb{E}[x+z]=\mathbb{E}[x]+\mathbb{E}[z]$
\item $\textrm{var}[x+z]=\textrm{var}[x]+\textrm{var}[z]$
\end{enumerate}

\subsection{Solution 1}
Assuming the variables to be continuous, we have:
\begin{equation}
\mathbb{E}[x+z]=\int\int f(x,z)(x+z)dx\ dz
\end{equation}
But since the variables are independent this is:
\begin{align}
\mathbb{E}[x+z]=&\int\int f(x)f(z)(x+z)dx\ dz=\\
&\int\int f(x)f(z)x\ dx\ dz+\int\int f(x)f(z)z\ dx\ dz=\\
&\int f(z)\int f(x)x\ dx\ dz+\int f(x)\int f(z)z\ dz\ dx=\\
&\int f(z)\mathbb{E}[x]dz+\int f(x)\mathbb{E}[z]dx=\\
&\mathbb{E}[x]\int f(z)dz+\mathbb{E}[z]\int f(x)dx=\mathbb{E}[x]+\mathbb{E}[z]
\end{align}
Similarly for discrete variables, where the integrals are replaced by sums.

\subsection{Solution 2}
Using this result, the variance is:
\begin{equation}
\textrm{var}[x+z]=\mathbb{E}[(x+z)^2]-\mathbb{E}[x+z]^2=\mathbb{E}[x^2]+\mathbb{E}[z^2]+2\mathbb{E}[xz]-\mathbb{E}[x+z]^2
\end{equation}
But we also know that:
\begin{equation}
\mathbb{E}[x+z]^2=(\mathbb{E}[x]+\mathbb{E}[z])^2=\mathbb{E}[x]^2+\mathbb{E}[z]^2+2\mathbb{E}[x]\mathbb{E}[z]
\end{equation}
So:
\begin{equation}
\textrm{var}[x+z]=\textrm{var}[x]+\textrm{var}[z]+2(\mathbb{E}[xz]-\mathbb{E}[x]\mathbb{E}[z])
\end{equation}
So if we can show $\mathbb{E}[xz]=\mathbb{E}[x]\mathbb{E}[z]$ we're done:
\begin{align}
\mathbb{E}[xz]=\int\int f(x,z)xz\ dx\ dz&=\\
\int\int f(x)f(z)xz\ dx\ dz&=
\int f(x)x\ dx\int f(z)z\ dz=\mathbb{E}[x]\mathbb{E}[z]
\end{align}

\end{document}
